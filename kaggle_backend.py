# VNeID Voice AI Backend - Kaggle Notebook
# Ch·∫°y tr√™n Kaggle v·ªõi GPU T4/P100

# ==========================================
# CELL 1: Install dependencies
# ==========================================

!pip install -q transformers accelerate bitsandbytes
!pip install -q openai-whisper
!pip install -q gradio
!pip install -q pyngrok

# ==========================================
# CELL 2: Load Models
# ==========================================

import torch
import whisper
from transformers import AutoModelForCausalLM, AutoTokenizer
import re
import json

print("üîÑ Loading Whisper model...")
whisper_model = whisper.load_model("medium")  # D√πng medium cho Kaggle (large c·∫ßn nhi·ªÅu RAM)
print("‚úÖ Whisper loaded!")

print("üîÑ Loading Vietnamese LLM...")
# D√πng PhoGPT cho nh·∫π h∆°n, ho·∫∑c Vistral n·∫øu ƒë·ªß RAM
model_name = "vinai/PhoGPT-4B-Chat"
# model_name = "vilm/Vistral-7B-Chat"  # Uncomment n·∫øu ƒë·ªß RAM

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # Quantization ƒë·ªÉ ti·∫øt ki·ªám RAM
)
print("‚úÖ LLM loaded!")

# ==========================================
# CELL 3: Define System Prompt & Functions
# ==========================================

SYSTEM_PROMPT = """B·∫°n l√† tr·ª£ l√Ω AI c·ªßa ·ª©ng d·ª•ng VNeID, gi√∫p ng∆∞·ªùi d√πng ƒëi·ªÅn form xin c·∫•p Phi·∫øu L√Ω l·ªãch T∆∞ ph√°p.

Form L√Ω l·ªãch T∆∞ ph√°p c√≥ c√°c tr∆∞·ªùng th√¥ng tin sau:
1. ho_ten: H·ªç v√† t√™n ƒë·∫ßy ƒë·ªß
2. ngay_sinh: Ng√†y th√°ng nƒÉm sinh (ƒë·ªãnh d·∫°ng DD/MM/YYYY)
3. cccd: S·ªë CƒÉn c∆∞·ªõc c√¥ng d√¢n (12 ch·ªØ s·ªë)
4. gioi_tinh: Gi·ªõi t√≠nh (Nam/N·ªØ)
5. que_quan: Qu√™ qu√°n
6. thuong_tru: N∆°i th∆∞·ªùng tr√∫
7. muc_dich: M·ª•c ƒë√≠ch xin c·∫•p (xin vi·ªác, du h·ªçc, k·∫øt h√¥n v·ªõi ng∆∞·ªùi n∆∞·ªõc ngo√†i, c·∫•p visa, b·ªï sung h·ªì s∆° c√¥ng ch·ª©c, kinh doanh c√≥ ƒëi·ªÅu ki·ªán, kh√°c)

Nhi·ªám v·ª• c·ªßa b·∫°n:
1. Ph√¢n t√≠ch nh·ªØng g√¨ ng∆∞·ªùi d√πng n√≥i v√† tr√≠ch xu·∫•t th√¥ng tin ph√π h·ª£p v·ªõi c√°c tr∆∞·ªùng tr√™n
2. Tr·∫£ l·ªùi B·∫∞NG TI·∫æNG VI·ªÜT, th√¢n thi·ªán, d·ªÖ hi·ªÉu (ng∆∞·ªùi d√πng c√≥ th·ªÉ l√† ng∆∞·ªùi gi√† ho·∫∑c ng∆∞·ªùi khuy·∫øt t·∫≠t)
3. Cu·ªëi c√¢u tr·∫£ l·ªùi, LU√îN k√®m theo JSON v·ªõi format: {"extracted": {...}, "missing": [...], "next_question": "..."}

V√≠ d·ª•:
- User: "T√¥i t√™n l√† Nguy·ªÖn VƒÉn An, sinh nƒÉm 1990"
- Assistant: "D·∫°, em ƒë√£ ghi nh·∫≠n anh/ch·ªã Nguy·ªÖn VƒÉn An, sinh nƒÉm 1990. Anh/ch·ªã cho em xin ng√†y th√°ng sinh c·ª• th·ªÉ v√† s·ªë cƒÉn c∆∞·ªõc c√¥ng d√¢n ƒë∆∞·ª£c kh√¥ng ·∫°?
{"extracted": {"ho_ten": "Nguy·ªÖn VƒÉn An", "ngay_sinh": "1990"}, "missing": ["ngay_sinh_day_month", "cccd", "gioi_tinh", "que_quan", "thuong_tru", "muc_dich"], "next_question": "ngay_sinh_day_month"}"

L∆∞u √Ω quan tr·ªçng:
- N·∫øu ng∆∞·ªùi d√πng n√≥i s·ªë, h√£y l·ªçc ra s·ªë (VD: "kh√¥ng m·ªôt hai ba" ‚Üí "0123")
- N·∫øu ng∆∞·ªùi d√πng n√≥i ƒë·ªãa ch·ªâ, format l·∫°i cho chu·∫©n
- S·ª≠ d·ª•ng ng√¥n ng·ªØ l·ªãch s·ª±, x∆∞ng h√¥ "em" v·ªõi ng∆∞·ªùi d√πng
"""

def process_audio(audio_path):
    """X·ª≠ l√Ω audio file v√† tr·∫£ v·ªÅ text + form data"""
    
    # 1. Speech-to-Text v·ªõi Whisper
    print("üé§ Transcribing audio...")
    result = whisper_model.transcribe(
        audio_path, 
        language="vi",
        task="transcribe"
    )
    transcript = result["text"].strip()
    print(f"üìù Transcript: {transcript}")
    
    if not transcript:
        return {
            "success": False,
            "error": "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c gi·ªçng n√≥i. Vui l√≤ng n√≥i to v√† r√µ h∆°n.",
            "transcript": ""
        }
    
    # 2. LLM x·ª≠ l√Ω ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh
    print("ü§ñ Processing with LLM...")
    
    messages = f"### C√¢u h·ªèi: {transcript}\n### Tr·∫£ l·ªùi:"
    
    inputs = tokenizer(
        SYSTEM_PROMPT + "\n\n" + messages, 
        return_tensors="pt"
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=300,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # L·∫•y ph·∫ßn sau "### Tr·∫£ l·ªùi:"
    if "### Tr·∫£ l·ªùi:" in response:
        response = response.split("### Tr·∫£ l·ªùi:")[-1].strip()
    
    print(f"üí¨ LLM Response: {response}")
    
    # 3. Extract JSON t·ª´ response
    form_data = extract_json_from_response(response)
    
    # 4. Clean response (b·ªè JSON ƒë·ªÉ hi·ªÉn th·ªã)
    clean_response = re.sub(r'\{[^}]+\}', '', response).strip()
    
    return {
        "success": True,
        "transcript": transcript,
        "response": clean_response,
        "form_data": form_data.get("extracted", {}),
        "missing_fields": form_data.get("missing", []),
        "next_question": form_data.get("next_question", "")
    }

def extract_json_from_response(text):
    """Extract JSON object t·ª´ LLM response"""
    try:
        # T√¨m JSON pattern trong text
        json_match = re.search(r'\{[^{}]*"extracted"[^{}]*\}', text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
    except:
        pass
    
    # Fallback: regex extraction
    data = {"extracted": {}, "missing": [], "next_question": ""}
    
    # Extract h·ªç t√™n
    name_match = re.search(r'(?:t√™n|h·ªç t√™n)[:\s]+([A-Z√Ä-·ª∏a-z√†-·ªπ\s]+?)(?:,|\.|sinh|s·ªë|$)', text, re.IGNORECASE)
    if name_match:
        data["extracted"]["ho_ten"] = name_match.group(1).strip().title()
    
    # Extract ng√†y sinh
    date_match = re.search(r'(\d{1,2})[/\-\.](\d{1,2})[/\-\.](\d{4})', text)
    if date_match:
        data["extracted"]["ngay_sinh"] = f"{date_match.group(1).zfill(2)}/{date_match.group(2).zfill(2)}/{date_match.group(3)}"
    
    # Extract CCCD
    cccd_match = re.search(r'(\d{12})', text)
    if cccd_match:
        data["extracted"]["cccd"] = cccd_match.group(1)
    
    return data

# ==========================================
# CELL 4: Create Gradio Interface
# ==========================================

import gradio as gr

def gradio_process(audio):
    """Wrapper function cho Gradio"""
    if audio is None:
        return "Vui l√≤ng ghi √¢m ho·∫∑c upload file audio", "{}"
    
    result = process_audio(audio)
    
    if result["success"]:
        output_text = f"""üìù **B·∫°n n√≥i:** {result['transcript']}

ü§ñ **AI tr·∫£ l·ªùi:** {result['response']}

üìã **Th√¥ng tin ƒë√£ nh·∫≠n:**
{json.dumps(result['form_data'], ensure_ascii=False, indent=2)}
"""
        return output_text, json.dumps(result, ensure_ascii=False, indent=2)
    else:
        return f"‚ùå L·ªói: {result['error']}", json.dumps(result, ensure_ascii=False, indent=2)

# T·∫°o Gradio interface
demo = gr.Interface(
    fn=gradio_process,
    inputs=[
        gr.Audio(
            sources=["microphone", "upload"],
            type="filepath",
            label="üé§ Ghi √¢m ho·∫∑c upload file audio"
        )
    ],
    outputs=[
        gr.Textbox(label="üìã K·∫øt qu·∫£", lines=10),
        gr.JSON(label="üîß Raw Response (ƒë·ªÉ debug)")
    ],
    title="üÜî VNeID Voice AI - Demo Backend",
    description="""
    ## H∆∞·ªõng d·∫´n test:
    1. Nh·∫•n n√∫t ghi √¢m üé§ ho·∫∑c upload file audio
    2. N√≥i ti·∫øng Vi·ªát, v√≠ d·ª•: "T√¥i t√™n l√† Nguy·ªÖn VƒÉn An, sinh ng√†y 15 th√°ng 3 nƒÉm 1990"
    3. Xem k·∫øt qu·∫£ AI tr√≠ch xu·∫•t th√¥ng tin
    
    ## C√°c c√¢u test m·∫´u:
    - "T√¥i t√™n l√† Nguy·ªÖn VƒÉn An, sinh ng√†y 15 th√°ng 3 nƒÉm 1990"
    - "S·ªë cƒÉn c∆∞·ªõc c·ªßa t√¥i l√† 012345678901"
    - "Qu√™ t√¥i ·ªü H√† N·ªôi, hi·ªán ƒëang ·ªü s·ªë 123 ph·ªë Hu·∫ø"
    - "T√¥i c·∫ßn l√Ω l·ªãch t∆∞ ph√°p ƒë·ªÉ xin vi·ªác"
    """,
    examples=[],
    cache_examples=False
)

# ==========================================
# CELL 5: Launch with Public URL
# ==========================================

from pyngrok import ngrok

# Set your ngrok authtoken (get from ngrok.com)
# ngrok.set_auth_token("YOUR_AUTH_TOKEN")

# Launch Gradio
print("üöÄ Starting server...")
demo.launch(share=True, debug=True)

# Ho·∫∑c n·∫øu d√πng ngrok:
# public_url = ngrok.connect(7860)
# print(f"üåê Public URL: {public_url}")
# demo.launch(server_port=7860)

# ==========================================
# CELL 6: API Endpoint (Optional - d√πng Flask)
# ==========================================

# N·∫øu c·∫ßn API thay v√¨ Gradio UI, uncomment code d∆∞·ªõi:

"""
from flask import Flask, request, jsonify
import tempfile
import os

app = Flask(__name__)

@app.route('/api/process_voice', methods=['POST'])
def api_process_voice():
    if 'audio' not in request.files:
        return jsonify({"success": False, "error": "No audio file"})
    
    audio_file = request.files['audio']
    
    # Save to temp file
    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
        audio_file.save(tmp.name)
        result = process_audio(tmp.name)
        os.unlink(tmp.name)
    
    return jsonify(result)

@app.route('/api/health', methods=['GET'])
def health():
    return jsonify({"status": "ok", "models_loaded": True})

# Run v·ªõi ngrok
from pyngrok import ngrok
public_url = ngrok.connect(5000)
print(f"üåê API URL: {public_url}")
app.run(port=5000)
"""
