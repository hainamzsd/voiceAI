{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# VNeID Voice AI Backend (Colab/Kaggle)\n\nRun this notebook on **Kaggle** or **Google Colab** with GPU enabled.\n\n**Setup:**\n1. Enable GPU: Runtime > Change runtime type > GPU (T4)\n2. Add your `CLAUDE_API_KEY` and `NGROK_AUTH_TOKEN` to Secrets\n3. Run cell 1 (Install) → **RESTART RUNTIME** → Run remaining cells\n4. Copy the ngrok URL to your React Native app\n\n**Important:** After installing dependencies, you MUST restart the runtime!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install dependencies\n# After running this cell, RESTART THE RUNTIME, then continue from Step 2\n\n!pip uninstall -y scipy -q 2>/dev/null\n!pip install -q numpy==1.26.4 scipy==1.12.0\n!pip install -q flask flask-cors pyngrok anthropic\n!pip install -q faster-whisper\n!pip install -q edge-tts  # Microsoft Edge TTS (free, Vietnamese support)\n\n# Try to install VieNeu-TTS (may fail due to dependencies)\ntry:\n    !pip install -q vieneu 2>/dev/null\n    print(\"VieNeu-TTS installed!\")\nexcept:\n    print(\"VieNeu-TTS skipped - will use Edge TTS\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"RESTART RUNTIME NOW!\")\nprint(\"Kaggle: Session > Restart Session\")\nprint(\"Colab: Runtime > Restart runtime\")\nprint(\"Then run from Step 2 (Configuration)\")\nprint(\"=\"*50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Configuration (run this AFTER restarting runtime)\nimport os\n\n# For Kaggle: Add secrets in \"Add-ons\" > \"Secrets\"\n# For Colab: Add secrets in the key icon on the left sidebar\n\ntry:\n    # Kaggle\n    from kaggle_secrets import UserSecretsClient\n    secrets = UserSecretsClient()\n    CLAUDE_API_KEY = secrets.get_secret(\"CLAUDE_API_KEY\")\n    NGROK_AUTH_TOKEN = secrets.get_secret(\"NGROK_AUTH_TOKEN\")\nexcept:\n    try:\n        # Colab\n        from google.colab import userdata\n        CLAUDE_API_KEY = userdata.get('CLAUDE_API_KEY')\n        NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n    except:\n        # Manual input (replace with your keys)\n        CLAUDE_API_KEY = \"your-claude-api-key-here\"\n        NGROK_AUTH_TOKEN = \"your-ngrok-token-here\"\n\n# Model settings\nWHISPER_MODEL = \"base\"  # tiny, base, small, medium, large-v3\nVIENEU_VOICE = \"Binh\"\n\nprint(f\"Claude API: {'OK' if CLAUDE_API_KEY and CLAUDE_API_KEY != 'your-claude-api-key-here' else 'MISSING'}\")\nprint(f\"Ngrok: {'OK' if NGROK_AUTH_TOKEN and NGROK_AUTH_TOKEN != 'your-ngrok-token-here' else 'MISSING'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Load models\nimport torch\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only'}\")\n\n# Load Whisper\nprint(f\"\\nLoading Whisper ({WHISPER_MODEL})...\")\nfrom faster_whisper import WhisperModel\nwhisper_model = WhisperModel(\n    WHISPER_MODEL,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    compute_type=\"float16\" if torch.cuda.is_available() else \"int8\"\n)\nprint(\"Whisper loaded!\")\n\n# Try VieNeu-TTS first, fallback to Edge TTS\nUSE_VIENEU = False\nvieneu_tts = None\nvieneu_voice = None\n\ntry:\n    print(f\"\\nTrying VieNeu-TTS...\")\n    from vieneu import Vieneu\n    vieneu_tts = Vieneu()\n    vieneu_voice = vieneu_tts.get_preset_voice(VIENEU_VOICE)\n    USE_VIENEU = True\n    print(f\"VieNeu-TTS loaded! Voices: {vieneu_tts.list_preset_voices()}\")\nexcept Exception as e:\n    print(f\"VieNeu-TTS failed: {e}\")\n    print(\"Using Edge TTS (Microsoft) as fallback\")\n    import edge_tts\n    USE_VIENEU = False\n\nprint(f\"\\nTTS Engine: {'VieNeu-TTS' if USE_VIENEU else 'Edge TTS (vi-VN-HoaiMyNeural)'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Backend server code\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nimport anthropic\nimport numpy as np\nimport base64\nimport tempfile\nimport json\nimport re\nimport io\nimport wave\nimport asyncio\n\napp = Flask(__name__)\nCORS(app)\n\nclaude = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\nconversation_history = []\nMAX_HISTORY = 10\n\n\ndef get_system_prompt(user_context, screen_context):\n    user_info = \"\"\n    if user_context:\n        user_info = f\"\"\"\nTHONG TIN CONG DAN (DA XAC THUC - KHONG HOI LAI):\n- Ho ten: {user_context.get('hoTen', 'N/A')}\n- So CCCD: {user_context.get('cccd', 'N/A')}\n- Ngay sinh: {user_context.get('ngaySinh', 'N/A')}\n\"\"\"\n\n    screen_info = \"\"\n    if screen_context:\n        screen_name = screen_context.get('screen_name', 'home')\n        step = screen_context.get('current_step', 0)\n        actions = screen_context.get('available_actions', [])\n        screen_info = f\"\"\"\nMAN HINH: {screen_name}\nBuoc hien tai: {step}\nThao tac kha dung: {', '.join(actions)}\n\"\"\"\n\n    user_name = \"ong/ba\"\n    if user_context and user_context.get('hoTen'):\n        name_parts = user_context.get('hoTen', '').split()\n        if name_parts:\n            user_name = f\"ong/ba {name_parts[-1]}\"\n\n    return f\"\"\"Ban la can bo huong dan thu tuc hanh chinh tai bo phan mot cua.\n\n{user_info}\n{screen_info}\n\nNGUYEN TAC GIAO TIEP:\n1. Van phong hanh chinh, lich su, chuyen nghiep\n2. Xung \"toi\", goi nguoi dan la \"{user_name}\"\n3. KHONG hoi lai thong tin da co\n4. KHONG lap lai noi dung da noi\n5. KHONG noi cau thua nhu \"vang a\", \"da a\", \"duoc a\", \"de toi\"\n6. Khi da hieu yeu cau -> thuc hien ngay, khong hoi lai\n\nCACH TRA LOI:\n- Voi LENH/YEU CAU don gian (lam LLTP, tiep tuc, quay lai...): Tra loi 1 cau ngan + thuc hien\n- Voi CAU HOI can giai thich (LLTP la gi? Can giay to gi? Bao lau?...): Tra loi DAY DU, RO RANG, CHINH XAC\n\nVI DU TRA LOI NGAN (lenh hanh dong):\n- \"lam ly lich tu phap\" -> \"Toi mo form dang ky.\" + ACTION\n- \"2 ban\" -> \"Da ghi nhan.\" + ACTION  \n- \"tiep\" -> \"Chuyen buoc tiep.\" + ACTION\n\nVI DU TRA LOI DAY DU (cau hoi):\n- \"Ly lich tu phap la gi?\" -> \"Ly lich tu phap la van ban ghi lai thong tin ve tien an, tien su cua cong dan. Day la giay to bat buoc khi xin viec, du hoc, dinh cu nuoc ngoai, hoac lam ho so phap ly.\"\n- \"Can giay to gi?\" -> \"De lam LLTP, {user_name} can chuan bi: 1. CCCD/CMND ban chinh, 2. Ho khau hoac giay xac nhan cu tru. Neu lam ho nguoi khac can them giay uy quyen co cong chung.\"\n- \"Mat bao lau?\" -> \"Thoi gian xu ly tu 10-15 ngay lam viec ke tu ngay nop ho so hop le. Truong hop cap dac biet la 3 ngay.\"\n\nKHONG tra loi kieu:\n- \"Vang a, toi se giup...\"\n- \"Da, de toi xem...\"\n- Lap lai cau hoi cua nguoi dan\n\nKHI CAN THUC HIEN THAO TAC, them JSON cuoi cau:\n@@ACTION@@{{\"action\": \"ten_action\", \"data\": {{}}}}@@END@@\n\nCAC ACTION:\n- navigate_lltp: Mo form Ly lich tu phap\n- navigate_home: Ve trang chu  \n- next_step: Chuyen buoc tiep\n- prev_step: Quay lai\n- fill_field: Dien thong tin (data: muc_dich, so_ban, loai_phieu)\n- submit: Nop ho so\n\"\"\"\n\n\ndef transcribe(audio_path):\n    segments, _ = whisper_model.transcribe(audio_path, language=\"vi\", beam_size=5, vad_filter=True)\n    return \" \".join([s.text for s in segments]).strip()\n\n\ndef text_to_speech_vieneu(text):\n    \"\"\"TTS using VieNeu-TTS\"\"\"\n    audio = vieneu_tts.infer(text=text, voice=vieneu_voice, temperature=1.0, top_k=50)\n    if audio.dtype in [np.float32, np.float64]:\n        audio = (audio * 32767).astype(np.int16)\n    \n    buffer = io.BytesIO()\n    with wave.open(buffer, 'wb') as wav:\n        wav.setnchannels(1)\n        wav.setsampwidth(2)\n        wav.setframerate(24000)\n        wav.writeframes(audio.tobytes())\n    buffer.seek(0)\n    return base64.b64encode(buffer.read()).decode('utf-8')\n\n\nasync def text_to_speech_edge_async(text):\n    \"\"\"TTS using Edge TTS (async)\"\"\"\n    import edge_tts\n    \n    communicate = edge_tts.Communicate(text, \"vi-VN-HoaiMyNeural\")\n    buffer = io.BytesIO()\n    \n    async for chunk in communicate.stream():\n        if chunk[\"type\"] == \"audio\":\n            buffer.write(chunk[\"data\"])\n    \n    buffer.seek(0)\n    return base64.b64encode(buffer.read()).decode('utf-8')\n\n\ndef text_to_speech_edge(text):\n    \"\"\"TTS using Edge TTS (sync wrapper)\"\"\"\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        return loop.run_until_complete(text_to_speech_edge_async(text))\n    finally:\n        loop.close()\n\n\ndef text_to_speech(text):\n    \"\"\"Convert text to speech\"\"\"\n    if not text:\n        return None\n    try:\n        if USE_VIENEU:\n            return text_to_speech_vieneu(text)\n        else:\n            return text_to_speech_edge(text)\n    except Exception as e:\n        print(f\"TTS Error: {e}\")\n        return None\n\n\ndef process_with_claude(text, user_context, screen_context):\n    global conversation_history\n    conversation_history.append({\"role\": \"user\", \"content\": text})\n    if len(conversation_history) > MAX_HISTORY * 2:\n        conversation_history = conversation_history[-MAX_HISTORY * 2:]\n\n    try:\n        response = claude.messages.create(\n            model=\"claude-3-5-haiku-20241022\",\n            max_tokens=300,\n            system=get_system_prompt(user_context, screen_context),\n            messages=conversation_history\n        )\n        result = response.content[0].text\n        conversation_history.append({\"role\": \"assistant\", \"content\": result})\n\n        action, data = None, {}\n        match = re.search(r'@@ACTION@@(.+?)@@END@@', result, re.DOTALL)\n        if match:\n            try:\n                action_json = json.loads(match.group(1).strip())\n                action = action_json.get('action')\n                data = action_json.get('data', {})\n            except:\n                pass\n\n        clean_text = re.sub(r'@@ACTION@@.*?@@END@@', '', result, flags=re.DOTALL).strip()\n        return clean_text, action, data\n    except Exception as e:\n        print(f\"Claude Error: {e}\")\n        return \"He thong gap loi. Vui long thu lai.\", None, {}\n\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\n        \"status\": \"healthy\", \n        \"gpu\": torch.cuda.is_available(),\n        \"tts\": \"vieneu\" if USE_VIENEU else \"edge\"\n    })\n\n\n@app.route('/reset', methods=['POST'])\ndef reset():\n    global conversation_history\n    conversation_history = []\n    return jsonify({\"status\": \"ok\"})\n\n\n@app.route('/process_text', methods=['POST'])\ndef process_text():\n    try:\n        data = request.json\n        text = data.get('text', '')\n        user_context = data.get('user_context')\n        screen_context = data.get('screen_context')\n\n        if not text:\n            return jsonify({\"success\": False, \"error\": \"No text\"})\n\n        response_text, action, action_data = process_with_claude(text, user_context, screen_context)\n        audio_b64 = text_to_speech(response_text)\n\n        return jsonify({\n            \"success\": True,\n            \"transcript\": text,\n            \"response\": response_text,\n            \"audio\": audio_b64,\n            \"action\": action,\n            \"data\": action_data,\n        })\n    except Exception as e:\n        return jsonify({\"success\": False, \"error\": str(e)})\n\n\n@app.route('/process_voice', methods=['POST'])\ndef process_voice():\n    try:\n        if 'audio' not in request.files:\n            return jsonify({\"success\": False, \"error\": \"No audio\"})\n\n        audio_file = request.files['audio']\n        user_context = request.form.get('user_context')\n        screen_context = request.form.get('screen_context')\n\n        if user_context:\n            user_context = json.loads(user_context)\n        if screen_context:\n            screen_context = json.loads(screen_context)\n\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp:\n            audio_file.save(tmp.name)\n            tmp_path = tmp.name\n\n        transcript = transcribe(tmp_path)\n        os.unlink(tmp_path)\n\n        print(f\"Transcript: {transcript}\")\n\n        if not transcript:\n            no_hear = \"Khong nghe ro. Vui long noi lai.\"\n            return jsonify({\n                \"success\": True,\n                \"transcript\": \"\",\n                \"response\": no_hear,\n                \"audio\": text_to_speech(no_hear),\n                \"action\": None,\n                \"data\": {},\n            })\n\n        response_text, action, action_data = process_with_claude(transcript, user_context, screen_context)\n        audio_b64 = text_to_speech(response_text)\n\n        return jsonify({\n            \"success\": True,\n            \"transcript\": transcript,\n            \"response\": response_text,\n            \"audio\": audio_b64,\n            \"action\": action,\n            \"data\": action_data,\n        })\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return jsonify({\"success\": False, \"error\": str(e)})\n\n\nprint(\"Flask app ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Start ngrok tunnel and run server\nfrom pyngrok import ngrok\nimport threading\n\n# Set ngrok auth token\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\n\n# Start ngrok tunnel\npublic_url = ngrok.connect(5000)\nprint(\"=\" * 60)\nprint(\"VNeID Voice AI Backend is running!\")\nprint(\"=\" * 60)\nprint(f\"\\n>>> PUBLIC URL: {public_url}\")\nprint(f\"\\nCopy this URL to your React Native app's BACKEND_URL\")\nprint(\"=\" * 60)\n\n# Run Flask in a thread\ndef run_flask():\n    app.run(host='0.0.0.0', port=5000, threaded=True, use_reloader=False)\n\nflask_thread = threading.Thread(target=run_flask)\nflask_thread.start()\n\nprint(\"\\nServer is running! Keep this notebook open.\")\nprint(\"Check /health endpoint to verify:\", f\"{public_url}/health\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 6 (Optional): Test the endpoints\nimport requests\n\n# Test health\nr = requests.get(f\"{public_url}/health\")\nprint(\"Health:\", r.json())\n\n# Test text processing\nr = requests.post(f\"{public_url}/process_text\", json={\n    \"text\": \"Xin chao\",\n    \"user_context\": {\"hoTen\": \"Nguyen Van A\"},\n    \"screen_context\": {\"screen_name\": \"home\"}\n})\nresult = r.json()\nprint(\"Response:\", result.get('response'))\nprint(\"Has audio:\", \"Yes\" if result.get('audio') else \"No\")"
  },
  {
   "cell_type": "code",
   "source": "# Step 7: Keep the notebook running\n# Run this cell to keep the server alive\nimport time\nprint(\"=\"*60)\nprint(\"Server is running!\")\nprint(f\"URL: {public_url}\")\nprint(\"=\"*60)\nprint(\"\\nKeep this cell running. Press stop button to shutdown.\")\n\nwhile True:\n    time.sleep(60)\n    print(\".\", end=\"\", flush=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}